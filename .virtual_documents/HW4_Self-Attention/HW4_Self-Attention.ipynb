





!pip install torchviz


import pandas as pd 
import numpy as np
import random
from pathlib import Path

import torch
import torch.nn as nn 
from torch.utils.data import Dataset, random_split, DataLoader
from torch import functional  as F
from torch.optim import Optimizer
from torch.optim.lr_scheduler import LambdaLR
from torch.nn.utils.rnn import pad_sequence
import math

import os
import sys
import json
from tqdm import tqdm
# 绘制评估曲线
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt
from torchviz import make_dot

import warnings 
from rich.console import Console
warnings.filterwarnings('ignore')
cs = Console()








def model_plot(model_class, input_sample):
    clf = model_class()
    y = clf(input_sample) 
    clf_view = make_dot(y, params=dict(list(clf.named_parameters()) + [('x', input_sample)]))
    return clf_view


def all_seed(seed=6666):
    np.random.seed(seed)
    random.seed(seed)
    # CPU
    torch.manual_seed(seed)
    # GPU
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.cuda.manual_seed(seed)
    # python全局
    os.environ['PYTHONHASHSEED'] = str(seed)
    # cudnn
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.enabled = False
    print(f'Set env random_seed = {seed}')


all_seed(87)





# 查看数据
data_dir = 'E:\Voxceleb2Dataset'

# mapping.json
map_ = Path(data_dir) / 'mapping.json'
map_js = json.load(map_.open())
cs.print('mapping.json | keys = ', map_js.keys())
# metadata.json 
matedata_ = Path(data_dir) / 'metadata.json'
matedata_js = json.load(matedata_.open())
cs.print('metadata.json | keys = ', matedata_js.keys())
cs.print("matedata_js['n_mels']=", matedata_js['n_mels'])
cs.print(
    "matedata_js['speakers']['id00559'][:5]=", 
    matedata_js['speakers']['id00559'][:5]
)

mel = torch.load(os.path.join(data_dir, 'uttr-2918eae600684146903d49f02275cb94.pt'))
cs.print(f'(mel_len, n_mels)={mel.shape}') 
mel


class myDataset(Dataset):
    def __init__(self, data_dir, segment_len=128):
        super(myDataset, self).__init__()
        self.data_dir = data_dir
        self.segment_len = segment_len
        # 加载演讲者和id编码的映射.
        mapping_path = Path(data_dir) / 'mapping.json'
        mapping = json.load(mapping_path.open())
        self.speaker2id = mapping['speaker2id']
        
        # 加载训练数据的源数据(特征文件， 演讲者)
        metadata_path = Path(data_dir) / 'metadata.json'
        metadata = json.load(metadata_path.open())['speakers']
        
        # 获取总演讲者数
        self.speaker_num = len(metadata.keys())
        self.data = []
        for speaker, utt in metadata.items():
            for utt_i in utt:
                self.data.append([utt_i['feature_path'], self.speaker2id[speaker]])

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        feat_path, speaker = self.data[index]
        # 载入经过预处理的mel图谱特征(mel-spectrogram)
        mel = torch.load(os.path.join(self.data_dir, feat_path))
        # 分割 mel-specrogram
        if len(mel) > self.segment_len:
            # 开始的位置为随机
            start = random.randint(0, len(mel) - self.segment_len)
            # 切分语音
            mel = torch.FloatTensor(mel[start: start + self.segment_len])
        else:
            mel = torch.FloatTensor(mel)
        # 将speaker 转成long格式便于后续计算loss
        speaker = torch.FloatTensor([speaker]).long()
        return mel, speaker
    
    def get_speaker_number(self):
        return self.speaker_num


class InferenceDataset(Dataset):
    def __init__(self, data_dir):
        super(InferenceDataset, self).__init__()
        test_path = Path(data_dir) / 'testdata.json'
        metadata = json.load(test_path.open())
        self.data_dir = data_dir 
        self.data = metadata['utterances'] 
    
    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        utt = self.data[index]
        feat_path = utt['feature_path']
        mel = torch.load(os.path.join(self.data_dir, feat_path))
        return feat_path, mel
    
    
def inference_collate_batch(batch):
    feat_path, mels = zip(*batch)
    return feat_path, torch.stack(mels)





class Classifier(nn.Module):
    def __init__(self, input_dim=40, d_model=80, n_spks=600, dropout=0.1):
        super(Classifier, self).__init__()
        self.pre_net = nn.Linear(input_dim, d_model)
        # TODO:
        #   尝试改变Transformer， 改成Conformer.
        #   https://arxiv.org/abs/2005.08100 
        self.encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,  # self_attn [Q, K, V] shape=(d_model*3, d_model)
            dim_feedforward=256,
            nhead=2, 
            batch_first=True,
            activation='gelu'
        )
        self.pred_layer = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, n_spks)
        )
    
    def forward(self, mels):
        # out: (batch_size, length, d_model)
        out = self.pre_net(mels)
        out = self.encoder_layer(out)
        # mean pooling
        stats = out.mean(dim=1)
        return self.pred_layer(stats)


# x = torch.randn(1, 100, 40).requires_grad_(True)
# model_plot(Classifier, x)





def plot_lr():
    # 定义热身步数，即在初始阶段学习率逐渐增加到设定值的步数
    num_warmup_steps = 1000
    # 定义总的训练步数
    num_training_steps = 70000
    # 初始学习率
    lr = 0.01
    # 创建一个列表，用于存储每个步数对应的学习率
    res_list = []

    # 遍历从0到num_training_steps-1的所有训练步数
    for current_step in range(70000):
        # 如果当前步数小于热身步数，则线性增加学习率
        if current_step < num_warmup_steps:
            # 计算当前步数占热身步数的比例，并乘以初始学习率
            res = float(current_step) / float(max(1, num_warmup_steps))
            # 将计算出的学习率添加到列表中
            res_list.append(res * lr)
            # 继续下一次循环
            continue
        # 如果当前步数大于等于热身步数，则按照余弦衰减函数来调整学习率
        progress = float(current_step - num_warmup_steps) / float(
                    max(1, num_training_steps - num_warmup_steps)
                )
        # 应用余弦衰减公式计算学习率
        res = 0.5 * (1.0 + math.cos(math.pi * float(0.5) * 2.0 * progress))
        # 将计算出的学习率添加到列表中
        res_list.append(res * lr)

    # 使用matplotlib库绘制学习率的变化趋势图
    plt.plot(res_list)
    # 设置图表的标题，显示热身步数和总训练步数
    plt.title(f'Trend of Learning Rate\nnum_warmup_steps={num_warmup_steps}\nnum_training_steps={num_training_steps}')
    # 显示图表
    plt.show()

# 调用函数以执行绘图
plot_lr()


def get_cosine_schedule_with_warmup(
    opt: Optimizer,
    num_warmup_steps: int,
    num_training_steps: int,
    num_cycles: float = 0.5,
    last_epoch: int = -1
):
    """
    创建一个学习率变化策略,
    学习率跟随cosine值变化,
    在warm up时间段内变化区间在:
        0 -> 优化器设置的学习率 .
    Args:
        opt (Optimizer): 优化器类
        num_warmup_steps (int): 多少步增加一下lr
        num_training_steps (int): 总训练步骤
        num_cycles (float, optional): 变化周期. 默认为 0.5.
        last_epoch (int, optional): _description_. Defaults to -1.
    """
    def lr_lambda(current_step):
        # warmup
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        # 衰减
        progress = float(current_step - num_warmup_steps) / float(
            max(1, num_training_steps - num_warmup_steps)
        )
        return max(
            0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))
        )
    return LambdaLR(opt, lr_lambda, last_epoch)





def trainer(train_loader, valid_loader, model, config, device, rest_net_flag=False):
    # 对于分类任务, 我们常用cross-entropy评估模型表现.
    criterion = nn.CrossEntropyLoss()
    # 初始化优化器
    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate']) 
    if config['scheduler_flag']:
        scheduler = get_cosine_schedule_with_warmup(optimizer, config['warmup_steps'], len(train_loader) * config['n_epochs'])
    # 模型存储位置
    save_path =  config['save_path']

    writer = SummaryWriter()
    if not os.path.isdir('./models'):
        os.mkdir('./models')

    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0
    for epoch in range(n_epochs):
        model.train()
        loss_record = []
        train_accs = []
        train_pbar = tqdm(train_loader, position=0, leave=True)

        for x, y in train_pbar:
            optimizer.zero_grad()             
            x, y = x.to(device), y.to(device)  
            pred = model(x)
            loss = criterion(pred, y)
            loss.backward()
            optimizer.step()
            if config['scheduler_flag']:
                scheduler.step()
            step += 1
            acc = (pred.argmax(dim=-1) == y.to(device)).float().mean()
            l_ = loss.detach().item()
            loss_record.append(l_)
            train_accs.append(acc.detach().item())
            train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')
            train_pbar.set_postfix({'loss': f'{l_:.5f}', 'acc': f'{acc:.5f}'})
        
        
        mean_train_acc = sum(train_accs) / len(train_accs)
        mean_train_loss = sum(loss_record)/len(loss_record)
        writer.add_scalar('Loss/train', mean_train_loss, step)
        writer.add_scalar('ACC/train', mean_train_acc, step)
        
        model.eval() # 设置模型为评估模式
        loss_record = []
        test_accs = []
        for x, y in valid_loader:
            x, y = x.to(device), y.to(device)
            with torch.no_grad():
                pred = model(x)
                loss = criterion(pred, y)
                acc = (pred.argmax(dim=-1) == y.to(device)).float().mean()

            loss_record.append(loss.item())
            test_accs.append(acc.detach().item())
            
        mean_valid_acc = sum(test_accs) / len(test_accs)
        mean_valid_loss = sum(loss_record)/len(loss_record)
        print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f},acc: {mean_train_acc:.4f} Valid loss: {mean_valid_loss:.4f},acc: {mean_valid_acc:.4f} ')
        writer.add_scalar('Loss/valid', mean_valid_loss, step)
        writer.add_scalar('ACC/valid', mean_valid_acc, step)
        if mean_valid_loss < best_loss:
            best_loss = mean_valid_loss
            torch.save(model.state_dict(), save_path) # 保存最优模型
            print('Saving model with loss {:.3f}...'.format(best_loss))
            early_stop_count = 0
        else: 
            early_stop_count += 1

        if early_stop_count >= config['early_stop']:
            print('\nModel is not improving, so we halt the training session.')
            return





device = 'cuda' if torch.cuda.is_available() else 'cpu'
config = {
    'seed': 87,
    'dataset_dir': "E:\Voxceleb2Dataset",
    'n_epochs': 10,   # 35 
    'batch_size': 8,  # 64
    
    'scheduler_flag': True,
    'valid_steps': 2000,
    'warmup_steps': 1000,
    # 'total_steps': 70000, # len(train) * n_epochs
    'learning_rate': 1e-3,          
    'early_stop': 300,
    'n_workers': 8,
    'save_path': './models/model.ckpt'
}
print(device)
all_seed(config['seed'])





def collate_batch(batch):
    # 将一个batch中的数据合并
    """Collate a batch of data."""
    mel, speaker = zip(*batch)
    # 为了保持一个batch内的长度都是一样的所有需要进行padding, 同时设置batch的维度是最前面的一维
    mel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) 一个很小的值
    # mel: (batch size, length, 40)
    return mel, torch.FloatTensor(speaker).long()


data_dir = config['dataset_dir']
dataset = myDataset(data_dir)
speaker_num = dataset.get_speaker_number()
speaker2id = dataset.speaker2id
# 将数据拆分成训练集和验证集
trainlen = int(0.9 * len(dataset))
lengths = [trainlen, len(dataset) - trainlen]
trainset, validset = random_split(dataset, lengths)
testset = InferenceDataset(data_dir)

train_loader = DataLoader(
    trainset,
    batch_size=config['batch_size'],
    shuffle=True,
    drop_last=True,
    num_workers=config['n_workers'],
    pin_memory=True,
    collate_fn=collate_batch,
)

valid_loader = DataLoader(
    validset,
    batch_size=config['batch_size'],
    num_workers=config['n_workers'],
    drop_last=True,
    pin_memory=True,
    collate_fn=collate_batch,
)


test_loader = DataLoader(
    testset,
    batch_size=1,
    num_workers=config['n_workers'],
    shuffle=False,
    drop_last=False,
    pin_memory=True,
    collate_fn=inference_collate_batch,
)





model = Classifier(
    input_dim=40,  # n_mel
    d_model=80,
    n_spks=600, 
    dropout=0.1
).to(device)
trainer(train_loader, valid_loader, model, config, device)





model_best = Classifier().to(device)
model_best.load_state_dict(torch.load(config['save_path']))
model_best.eval()
mapping_path = Path(data_dir) / "mapping.json"
mapping = json.load(mapping_path.open())
pred_id = []
pred_final_cls = []
with torch.no_grad():
    for name, data in tqdm(test_loader):
        test_pred = model_best(data.to(device))
        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)
        pred_id += name
        pred_final_cls += [mapping["id2speaker"][str(test_label[0])]]


df = pd.DataFrame()
df["Id"] = pred_id
df["Category"] = pred_final_cls
df.to_csv("submission.csv",index = False)



