


# 必要环境配置与包安转
# !apt update
!pip uninstall gym -y
!pip install gym==0.25.2
# !apt install python-opengl xvfb -y
!pip install PyOpenGL PyOpenGL_accelerate
!pip install swig
!pip install box2d box2d-kengz
!pip install gym[box2d] pyvirtualdisplay


# 加载需要的包
from pyvirtualdisplay import Display
import matplotlib.pyplot as plt
from IPython import display
import numpy as np
import pandas as pd
import torch
from torch import nn
from torch import optim
from torch.nn import functional as F
import typing as typ
from torch.distributions import Categorical
# from tqdm.auto import tqdm
from tqdm import tqdm # 下载notebook 仍能显示
import gym
import random
import os
from rich.console import Console
import warnings

warnings.filterwarnings('ignore')
cs = Console()
# PyVirtualDisplay使用Xvfb作为显示，而Xvfb是X Window系统的无头显示服务器，Windows不使用X Window系统。
# v_dispaly = Display(visible=0, size=(1400, 900))   
# v_dispaly.start()
print("gym.__version__=", gym.__version__)


# 设置seed便于复现
NOTEBOOK_SEED = 543


def all_seed(seed=6666, env=None):
    if env is not None:
        env.seed(seed)
        env.action_space.seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    # CPU
    torch.manual_seed(seed)
    # GPU
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.cuda.manual_seed(seed)
    # python全局
    os.environ['PYTHONHASHSEED'] = str(seed)
    # cudnn
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.enabled = False
    print(f'Set env random_seed = {seed}')





def gym_env_desc(env_name):
    """
    对环境的简单描述
    """
    # 创建一个环境对象，这里env_name是环境的名称
    env = gym.make(env_name)
    # 获取环境的观察空间形状，对于大多数环境来说，这将返回一个元组，表示状态空间的维度
    state_shape = env.observation_space.shape
    cs.print("observation_space:\n\t", env.observation_space)
    cs.print("action_space:\n\t", env.action_space)
    try:
        # 获取环境的动作空间大小，对于离散动作空间的环境，这将返回一个整数，表示可能动作的数量
        action_shape = env.action_space.n
        action_type = '离散'
        extra=''
    except Exception as e:
        # 尝试获取环境的动作空间形状，这在连续动作空间的环境上是可行的
        # 但是对于离散动作空间的环境，这是错误的，因为Discrete动作空间没有shape属性
        # 应该使用 env.action_space.n 来获取离散动作空间中可能动作的数量
        # 下面的代码在离散动作空间的环境上将会引发AttributeError
        action_shape = env.action_space.shape
        low_ = env.action_space.low[0]  # 连续动作的最小值
        up_ = env.action_space.high[0]  # 连续动作的最大值
        extra=f'<{low_} -> {up_}>'
        action_type = '连续'
    print(f'[ {env_name} ](state: {state_shape},action: {action_shape}({action_type} {extra}))')
    return 


env_name= 'LunarLander-v2'
gym_env_desc(env_name)
env = gym.make(env_name)





env = gym.make(env_name) # 如果gym.__version__==0.26.2: 只需要 gym.make(env_name, render_mode='rgb_array') 
all_seed(seed=NOTEBOOK_SEED, env=env)
env.reset()

img = plt.imshow(env.render(mode='rgb_array')) # 如果gym.__version__==0.26.2: 只需要 plt.imshow(env.render())
done = False
while not done:
    a = env.action_space.sample()
    n_state, reward, done, _ = env.step(a)
    img.set_data(env.render(mode='rgb_array')) # 如果gym.__version__==0.26.2: 只需要 img.set_data(env.render()) 
    display.display(plt.gcf())
    display.clear_output(wait=True)





class PolicyGradientNet(nn.Module):
    def __init__(self, state_dim: int, action_dim: int):
        super(PolicyGradientNet, self).__init__()
        self.q_net = nn.ModuleList([
            nn.ModuleDict({
                'linear': nn.Linear(state_dim, 32),
                # 在神经网络模型中定义一个线性层的激活函数
                # 使用双曲正切（Tanh）激活函数，它将输入压缩到[-1, 1]的范围内
                # 这有助于处理具有负值和正值的数据，并且能够保持梯度在输入的较大范围内不会饱和
                'linear_activation': nn.Tanh()
            }),
            nn.ModuleDict({
                'linear': nn.Linear(32, 32),
                'linear_activation': nn.Tanh()
            }),
            nn.ModuleDict({
                'linear': nn.Linear(32, action_dim),
                'linear_activation': nn.Softmax(dim=-1)
            })
        ])
        
    def forward(self, x):
        for layer in self.q_net:
            x = layer['linear_activation'](layer['linear'](x))
        return x





from torch.optim.lr_scheduler import StepLR
from typing import List, Dict, AnyStr
from functools import reduce

class REINFORCE():
    def __init__(self, state_dim: int, action_dim: int, lr: float=0.001, gamma: float=0.9, 
                 stepLR_step_size:int = 200,
                 stepLR_gamma:float = 0.1,
                 normalize_reward:bool=False):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.policy_net = PolicyGradientNet(state_dim, action_dim)
        self.policy_net.to(self.device)
        self.opt = optim.Adam(self.policy_net.parameters(), lr=lr)
        self.stepLR_step_size = stepLR_step_size
        self.stepLR_gamma = stepLR_gamma
        self.sche = StepLR(self.opt, step_size=self.stepLR_step_size, gamma=self.stepLR_gamma)
        self.gamma = gamma
        self.normalize_reward = normalize_reward
        self.training = True
    
    def train(self):
        self.training = True
        self.policy_net.train()

    def eval(self):
        self.training = False
        self.policy_net.eval()
        
    @torch.no_grad()
    def policy(self, state):
        """
        sample action
        """
        # 将当前状态转换为浮点数张量，并移动到GPU
        action_prob = self.policy_net(torch.FloatTensor(state).to(self.device))
        
        # 使用从策略网络输出的动作概率创建一个分类分布对象
        # Categorical分布适用于离散动作空间，其中每个动作都有相应的概率
        action_dist = Categorical(action_prob)
        
        # 从动作分布中随机采样一个动作
        # 这将根据动作概率选择一个动作，通常用于实现随机策略
        action = action_dist.sample()
        
        # 将采样得到的动作从计算图中分离出来，并将其转换为一个Python整数
        # 这样做是为了防止在计算梯度时回溯到这个动作
        # 然后将动作从GPU移回CPU，最后将张量转换为一个Python标量值
        return action.detach().cpu().item()


    def batch_update(self, batch_episode: List[Dict[AnyStr, List]]):
        for transition_dict in batch_episode:
            self.update(transition_dict)
        self.sche.step()

    def update(self, transition_dict):
        reward_list = transition_dict['rewards']
        state_list = transition_dict['states']
        action_list = transition_dict['actions']
        # 分数 normalize
        if self.normalize_reward:
            reward_list = (np.array(reward_list) - np.mean(reward_list)) / (np.std(reward_list) + 1e-9)
        Rt = 0
        self.opt.zero_grad()
        for i in reversed(range(len(reward_list))):  # 从最后一步算起
            reward = reward_list[i]
            state = torch.tensor([state_list[i]], dtype=torch.float).to(self.device)
            action = torch.tensor([action_list[i]]).unsqueeze(0).to(self.device)
            log_prob = torch.log(self.policy_net(state).gather(1, action.long()))
            # Rt = \sum_{i=t}^T \gamma ^ {i-t} r_i
            Rt = self.gamma * Rt + reward
            loss = -log_prob * Rt
            loss.backward()
        self.opt.step() 
        
    def save_model(self, model_dir):
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
        file_path = os.path.join(model_dir, 'policy_net.ckpt')
        torch.save(self.policy_net.state_dict(), file_path)

    def load_model(self, model_dir):
        file_path = os.path.join(model_dir, 'policy_net.ckpt')
        self.policy_net.load_state_dict(torch.load(file_path))






def train_on_policy(
    agent, 
    env, 
    num_batch=450,
    random_batch=2,
    episode_per_batch=3,
    episode_max_step=300,
    save_mdoel_dir='./check_point'
):
    """
    on policy 强化学习算法学习简单函数
    params:
        agent: 智能体
        env: 环境
        random_batch: 前N个batch用random Agent收集数据
        num_batch: 训练多少个batch
        episode_per_batch： 一个batch下多少个episode
        episode_max_step: 每个episode最大步数
        save_mdoel_dir: 模型保存的文件夹
    """
    EPISODE_PER_BATCH = episode_per_batch
    NUM_BATCH = num_batch
    RANDOM_BATCH = random_batch
    MAX_STEP = episode_max_step
    avg_total_rewards, avg_final_rewards, avg_total_steps = [], [], []
    agent.train()
    tq_bar = tqdm(range(RANDOM_BATCH + NUM_BATCH))
    # 初始化一个变量来存储最近的最佳值，初始设置为负无穷大
    # 这意味着任何实际的数值都会比初始值大，从而可以被更新为最佳值
    recent_best = -np.inf
    # 初始化一个变量来存储当前批次中的最佳值，初始同样设置为负无穷大
    # 类似于recent_best，这允许任何实际的数值成为当前批次中的最佳值
    batch_best = -np.inf

    all_rewards = []
    for batch in tq_bar:
        tq_bar.set_description(f"[ {batch+1}/{NUM_BATCH} ]")
        batch_recordes = []
        total_rewards = []
        total_steps = []
        final_rewards = []
        for ep in range(EPISODE_PER_BATCH):
            rec_dict = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': []}
            state = env.reset()
            total_reward, total_step = 0, 0
            while True:
                a = agent.policy(state)
                if batch < RANDOM_BATCH:
                    a = env.action_space.sample()

                n_state, reward, done, _ = env.step(a)
                # 收集每一步的信息
                rec_dict['states'].append(state)
                rec_dict['actions'].append(a)
                rec_dict['next_states'].append(n_state)
                rec_dict['rewards'].append(reward)
                rec_dict['dones'].append(done)
                state = n_state
                total_reward += reward
                total_step += 1
                if done or total_step > MAX_STEP:
                    # 一个episode结束后 收集相关信息
                    final_rewards.append(reward)
                    total_steps.append(total_step)
                    total_rewards.append(total_reward)
                    all_rewards.append(total_reward)
                    batch_recordes.append(rec_dict)
                    break

        avg_total_reward = sum(total_rewards) / len(total_rewards)
        avg_final_reward = sum(final_rewards) / len(final_rewards)
        avg_total_step = sum(total_steps) / len(total_steps)
        recent_batch_best = np.mean(all_rewards[-10:])
        avg_total_rewards.append(avg_total_reward)
        avg_final_rewards.append(avg_final_reward)
        avg_total_steps.append(avg_total_step)
        # 在进度条后面显示关注的信息
        tq_bar.set_postfix({
            "Total": f"{avg_total_reward: 4.1f}", 
            "Recent": f"{recent_batch_best: 4.1f}", 
            "RecentBest": f"{recent_best: 4.1f}", 
            "Final": f"{avg_final_reward: 4.1f}", 
            "Steps": f"{avg_total_step: 4.1f}"})
        agent.batch_update(batch_recordes)
        if avg_total_reward > batch_best and (batch > 4 + RANDOM_BATCH):
            batch_best = avg_total_reward
            agent.save_model(save_mdoel_dir + "_batchBest")
        if recent_batch_best > recent_best and (batch > 4 + RANDOM_BATCH):
            recent_best = recent_batch_best
            agent.save_model(save_mdoel_dir)
        
    return avg_total_rewards, avg_final_rewards, avg_total_steps





# test sample env
env_name= 'CartPole-v1'
gym_env_desc(env_name)
env = gym.make(env_name)
all_seed(seed=NOTEBOOK_SEED, env=env)
agent = REINFORCE(
    state_dim=env.observation_space.shape[0], 
    action_dim=env.action_space.n, 
    lr=0.01,
    gamma=0.8,
    stepLR_step_size=80
)
avg_total_rewards, avg_final_rewards, avg_total_steps = train_on_policy(
    agent, env, 
    num_batch=160,
    random_batch=2,
    episode_per_batch=5,
    episode_max_step=300,
    save_mdoel_dir='./check_point'
)





fig, axes = plt.subplots(1, 2, figsize=(16, 4))
axes[0].plot(avg_total_rewards, label='Total Rewards')
axes[0].plot(avg_final_rewards, label='Final Rewards')
axes[0].set_title(f'{env_name} - Rewards Curve')
axes[0].legend()
axes[1].plot(avg_total_steps, label='steps')
axes[1].set_title(f'{env_name} - Steps Curve')
axes[1].legend()
plt.show()





env_name= 'LunarLander-v2'
gym_env_desc(env_name)
env = gym.make(env_name)
all_seed(seed=NOTEBOOK_SEED, env=env)
agent = REINFORCE(
    state_dim=env.observation_space.shape[0], 
    action_dim=env.action_space.n, 
    lr=0.001,
    gamma=0.99,
    normalize_reward=True,
    stepLR_step_size = 150,
    stepLR_gamma = 0.75
)

avg_total_rewards, avg_final_rewards, avg_total_steps = train_on_policy(
    agent, env, 
    num_batch=800,
    random_batch=3,
    episode_per_batch=3,
    episode_max_step=300, 
    save_mdoel_dir='./check_point_LunarLander_REINFORCE'
)





fig, axes = plt.subplots(1, 2, figsize=(16, 4))
axes[0].plot(avg_total_rewards, label='Total Rewards')
axes[0].plot(avg_final_rewards, label='Final Rewards')
axes[0].set_title(f'{env_name} - Rewards Curve')
axes[0].legend()
axes[1].plot(avg_total_steps, label='steps')
axes[1].set_title(f'{env_name} - Steps Curve')
axes[1].legend()
plt.show()





def test_agent(agent, env, max_steps=None, plot_flag=True):
    NUM_OF_TEST = 5 # Do not revise this !!!
    test_total_reward = []
    action_list = []
    env_name_ = str(env).rsplit('<', 1)[-1].replace('>', '')
    for i in range(NUM_OF_TEST):
        actions = []
        state = env.reset()
        if plot_flag:
            img = plt.imshow(env.render(mode='rgb_array'))
            plt.title(f'Test {env_name_}\n[ {i+1} / {NUM_OF_TEST} ]')
        total_reward = 0
        print(f"Inintial rewards={total_reward}")
        done = False
        ep_step = 0
        while not done:
            action = agent.policy(state)
            actions.append(action)
            state, reward, done, _ = env.step(action)
            total_reward += reward
            ep_step += 1
            if plot_flag:
                img.set_data(env.render(mode='rgb_array'))
                display.display(plt.gcf())
                display.clear_output(wait=True)
            if max_steps is not None:
                done = max_steps < ep_step

        print(f'[ {i+1}/{NUM_OF_TEST} ] step: {ep_step} reward: {total_reward}')
        test_total_reward.append(total_reward)
        action_list.append(actions) # save the result of testing 
    return test_total_reward, action_list


env_name= 'LunarLander-v2'
all_seed(seed=NOTEBOOK_SEED, env=env)
agent.load_model('./check_point_LunarLander_REINFORCE')
agent.eval() 
test_total_reward, action_list = test_agent(agent, env, max_steps=200)


print(test_total_reward)
print(np.mean(test_total_reward))
print("Action list looks like ", action_list)
print("Action list's shape looks like ", np.shape(action_list))


distribution = {}
for actions in action_list:
    for action in actions:
        if action not in distribution.keys():
            distribution[action] = 1
        else:
            distribution[action] += 1
print(distribution)





PATH = "Action_List.npy"
np.save(PATH, np.array(action_list)) 

### 这是需要提交的文件！！！ 将测试结果下载到您的设备
# from google.colab import files
# files.download(PATH)





action_list = np.load(PATH,allow_pickle=True) # The action list you upload
env_name= 'LunarLander-v2'
all_seed(seed=NOTEBOOK_SEED, env=env)

test_total_reward = []
if len(action_list) != 5:
    print("Wrong format of file !!!")
    exit(0)
for idx, actions in enumerate(action_list):
    state = env.reset()
#     img = plt.imshow(env.render(mode='rgb_array'))
#     plt.title(f'Test {env_name}\n[ {idx+1} / 5 ]')
    total_reward = 0
    done = False
    for action in actions:
        state, reward, done, _ = env.step(action)
        total_reward += reward
#         img.set_data(env.render(mode='rgb_array'))
#         display.display(plt.gcf())
#         display.clear_output(wait=True)
        if done:
            break
    print(f"Your reward is : %.2f" % total_reward)
    test_total_reward.append(total_reward)

# 最终分数
print(f"Your final reward is : %.2f" % np.mean(test_total_reward))





class ValueNet(torch.nn.Module):
    def __init__(self, state_dim):
        super(ValueNet, self).__init__()
        self.v_net = nn.ModuleList([
            nn.ModuleDict({
                'linear': nn.Linear(state_dim, 32),
                'linear_activation': nn.ReLU()
            }),
            nn.ModuleDict({
                'linear': nn.Linear(32, 32),
                'linear_activation': nn.ReLU()
            })
        ])
        self.head = nn.Linear(32, 1)

    def forward(self, x):
        for layer in self.v_net:
            x = layer['linear_activation'](layer['linear'](x))
        return self.head(x)


from torch.optim.lr_scheduler import LinearLR

class ActorCritic:
    """
    REINFORCE with Baseline
    """
    def __init__(self, state_dim: int, action_dim: int, actor_lr: float=0.001, critic_lr: float=0.001, gamma: float=0.9, 
                 schedule_kwargs: dict = dict(
                     schedule_func_name='StepLR',
                     stepLR_step_size = 200,
                     stepLR_gamma = 0.1,
                 ),
                 normalize_reward:bool = False):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.actor = PolicyGradientNet(state_dim, action_dim)
        self.critic = ValueNet(state_dim)
        self.actor.to(self.device)
        self.critic.to(self.device)

        self.actor_opt = optim.Adam(self.actor.parameters(), lr=actor_lr)
        self.critic_opt = optim.Adam(self.critic.parameters(), lr=critic_lr)
        if schedule_kwargs['schedule_func_name'] == "StepLR":
            self.stepLR_step_size = schedule_kwargs['stepLR_step_size']
            self.stepLR_gamma = schedule_kwargs['stepLR_gamma']
            self.actor_sche = StepLR(self.actor_opt, step_size=self.stepLR_step_size, gamma=self.stepLR_gamma)
            self.critic_sche = StepLR(self.critic_opt, step_size=self.stepLR_step_size, gamma=self.stepLR_gamma)
        if schedule_kwargs['schedule_func_name'] == "LinearLR":
            self.end_factor = schedule_kwargs['end_factor']
            self.total_iters = schedule_kwargs['total_iters']
            self.actor_sche = LinearLR(self.actor_opt, start_factor=1, end_factor=self.end_factor, total_iters=self.total_iters)
            self.critic_sche = LinearLR(self.critic_opt, start_factor=1, end_factor=self.end_factor, total_iters=self.total_iters)
        
        self.gamma = gamma
        self.normalize_reward = normalize_reward
        self.training = True
        
    def train(self):
        self.training = True
        self.actor.train()
        self.critic.train()

    def eval(self):
        self.training = False
        self.actor.eval()
        self.critic.eval()
        
    @torch.no_grad()
    def policy(self, state):
        """
        sample action
        """
        action_prob = self.actor(torch.FloatTensor(state).to(self.device))
        action_dist = Categorical(action_prob)
        action = action_dist.sample()
        return action.detach().cpu().item()

    def batch_update(self, batch_episode: List[Dict[AnyStr, List]]):
        for transition_dict in batch_episode:
            self.update(transition_dict)
        self.actor_sche.step()
        self.critic_sche.step()

    def update(self, transition_dict):
        reward_list = transition_dict['rewards']
        state_list = transition_dict['states']
        action_list = transition_dict['actions']
        # 分数 normalize
        if self.normalize_reward:
            reward_list = (np.array(reward_list) - np.mean(reward_list)) / (np.std(reward_list) + 1e-9)
        Rt = 0
        self.actor_opt.zero_grad()
        self.critic_opt.zero_grad()
        for i in reversed(range(len(reward_list))):  # 从最后一步算起
            reward = reward_list[i]
            state = torch.tensor([state_list[i]], dtype=torch.float).to(self.device)
            action = torch.tensor([action_list[i]]).unsqueeze(0).to(self.device)
            log_prob = torch.log(self.actor(state).gather(1, action.long()))
            # Rt = \sum_{i=t}^T \gamma ^ {i-t} r_i
            Rt = self.gamma * Rt + reward
            At = Rt - self.critic(state)
            critic_loss = torch.square(-At)
            loss = -log_prob * At.detach()  
            critic_loss.backward() 
            loss.backward()
        self.actor_opt.step() 
        self.critic_opt.step()
    
    def save_model(self, model_dir):
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
        file_path = os.path.join(model_dir, 'actor.ckpt')
        torch.save(self.actor.state_dict(), file_path)
        file_path = os.path.join(model_dir, 'critic.ckpt')
        torch.save(self.critic.state_dict(), file_path)

    def load_model(self, model_dir):
        file_path = os.path.join(model_dir, 'actor.ckpt')
        self.actor.load_state_dict(torch.load(file_path))
        file_path = os.path.join(model_dir, 'critic.ckpt')
        self.critic.load_state_dict(torch.load(file_path))





env_name= 'LunarLander-v2'
gym_env_desc(env_name)
env = gym.make(env_name)
all_seed(seed=NOTEBOOK_SEED, env=env)

agent = ActorCritic(
    state_dim=env.observation_space.shape[0], 
    action_dim=env.action_space.n, 
    actor_lr=0.0025,
    critic_lr=0.0025, # 0.0017 -> 164   0.0025->263 rd 113
    gamma=0.99,
    normalize_reward=True,
#     schedule_kwargs=dict(
#          schedule_func_name='StepLR',
#     #     stepLR_step_size = 150,  # lr=0.001 episode_max_step=300
#     #     stepLR_gamma = 0.8  # StepLR | Your final reward is : 142.25
#         stepLR_step_size = 120, #  lr=0.001 episode_max_step=300
#         stepLR_gamma = 0.85 # StepLR | Your final reward is : 150.42     Recent=105.5, RecentBest=170.4, Final=-0.1, Steps=301.0]  
#      )
    schedule_kwargs=dict(
        schedule_func_name='LinearLR', 
#         end_factor = 0.1,
#         total_iters = 1000  # Your final reward is : 152.00
        end_factor = 0.1,
        total_iters = 1000
     )
)
avg_total_rewards, avg_final_rewards, avg_total_step = train_on_policy(
    agent, env, 
    num_batch=1200,
    random_batch=3,
    episode_per_batch=3, 
    episode_max_step=300, # 300
    save_mdoel_dir='./check_point_LunarLander-ActorCritic2'
)





fig, axes = plt.subplots(1, 2, figsize=(16, 4))
axes[0].plot(avg_total_rewards, label='Total Rewards')
axes[0].plot(avg_final_rewards, label='Final Rewards')
axes[0].set_title(f'{env_name} - Rewards Curve')
axes[0].legend()
axes[1].plot(avg_total_step, label='steps')
axes[1].set_title(f'{env_name} - Steps Curve')
axes[1].legend()
plt.show()





agent = ActorCritic(
    state_dim=env.observation_space.shape[0], 
    action_dim=env.action_space.n, 
    actor_lr=0.0025,
    critic_lr=0.0025,
    gamma=0.99,
    normalize_reward=True,
    schedule_kwargs=dict(
        schedule_func_name='LinearLR', 
        end_factor = 0.1,
        total_iters = 1000
     )
)

env_name= 'LunarLander-v2'
env = gym.make(env_name)
all_seed(seed=NOTEBOOK_SEED, env=env)
agent.load_model('./check_point_LunarLander-ActorCritic2')
agent.eval()
test_total_reward, action_list = test_agent(agent, env, max_steps=300, plot_flag=True)
print(test_total_reward)
print(np.mean(test_total_reward))


env_name= 'LunarLander-v2'
env = gym.make(env_name)
all_seed(seed=NOTEBOOK_SEED, env=env)
agent.load_model('./check_point_LunarLander-ActorCritic2')
agent.eval()
test_total_reward, action_list = test_agent(agent, env, max_steps=300, plot_flag=False)
print(test_total_reward)
print(np.mean(test_total_reward))





  
