{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 13 - 神经网络压缩(`Network Compression`)\n",
    "\n",
    "作者: Liang-Hsuan Tseng (b07502072@ntu.edu.tw), modified from ML2021-HW13  \n",
    "如果你有任何问题, 可以免费询问: ntu-ml-2022spring-ta@googlegroups.com  \n",
    "\n",
    "[**HW13 PPT**](https://docs.google.com/presentation/d/1nCT9XrInF21B4qQAWuODy5sonKDnpGhjtcAwqa75mVU/edit#slide=id.p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noteBook 目录\n",
    "\n",
    "* [Packages](#Packages) - 安转必要的一些包\n",
    "* [Configs](#Configs) - 实验的配置，你可以在这里更改一些超参数。\n",
    "* [Dataset](#Dataset) - 您需要了解的有关数据集的信息。\n",
    "* [Architecture_Design](#Architecture_Design) - 深度(`depthwise`)和逐点(`pointwise`)卷积示例以及一些有用的链接。  \n",
    "* [Knowledge_Distillation](#Knowledge_Distillation) - 在知识提炼中的KL离散损失和一些有用的链接。\n",
    "* [Training](#Training) - 从HW3修改的训练循环实现。\n",
    "* [Inference](#Inference) - 用训练产出的`student_best.ckpt`生成`submission.csv` 。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: torchsummary in g:\\anaconda3\\envs\\pytorch\\lib\\site-packages (1.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (g:\\anaconda3\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (g:\\anaconda3\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (g:\\anaconda3\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (g:\\anaconda3\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (g:\\anaconda3\\envs\\pytorch\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (g:\\anaconda3\\envs\\pytorch\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  2 15:23:05 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 546.33                 Driver Version: 546.33       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650      WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   42C    P8               3W /  50W |    384MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1316    C+G   ...3\\extracted\\runtime\\WeChatAppEx.exe    N/A      |\n",
      "|    0   N/A  N/A      1448    C+G   ...64__8wekyb3d8bbwe\\CalculatorApp.exe    N/A      |\n",
      "|    0   N/A  N/A      1880    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      4980    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      5980    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      6068    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9588    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     13404    C+G   ...nipaste-2.7.3-Beta-x64\\Snipaste.exe    N/A      |\n",
      "|    0   N/A  N/A     14084    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15340    C+G   ...on\\wallpaper_engine\\wallpaper32.exe    N/A      |\n",
      "|    0   N/A  N/A     16940    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     18608    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     21032    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     22020      C   G:\\Anaconda3\\envs\\pytorch\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     22164    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     26044    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     26292    C+G   G:\\QQNT\\QQ.exe                            N/A      |\n",
      "|    0   N/A  N/A     26676    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     30812    C+G   G:\\Soda Music\\1.6.9\\SodaMusic.exe         N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset # \"ConcatDataset\" 和 \"Subset\" 有可能使用\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "from torchsummary import summary\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# 查看GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs\n",
    "\n",
    "在本部分中，你可以指定一些变量和超参数作为配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_seed(seed=6666, env=None):\n",
    "    if env is not None:\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # CPU\n",
    "    torch.manual_seed(seed)\n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        \n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    # python全局\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    print(f'Set env random_seed = {seed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'dataset_root': 'E:\\deeplearning_dataset\\\\food11-hw13',\n",
    "    'save_dir': './outputs',\n",
    "    'exp_name': \"simple_baseline\",\n",
    "    'batch_size': 64,\n",
    "    'lr': 5e-4,\n",
    "    'seed': 20220013,\n",
    "    'loss_fn_type': 'KD', # simple baseline: CE, medium baseline: KD.\n",
    "    'weight_decay': 0, #1e-5,\n",
    "    'grad_norm_max': 10,\n",
    "    'n_epochs': 20, # 训练更多的步骤以通过中等基线(medium baseline).\n",
    "    'patience': 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set env random_seed = 20220013\n",
      "{'dataset_root': 'E:\\\\deeplearning_dataset\\\\food11-hw13', 'save_dir': './outputs', 'exp_name': 'simple_baseline', 'batch_size': 64, 'lr': 0.0005, 'seed': 20220013, 'loss_fn_type': 'KD', 'weight_decay': 0, 'grad_norm_max': 10, 'n_epochs': 20, 'patience': 300}\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子\n",
    "all_seed(cfg['seed'])\n",
    "save_path = os.path.join(cfg['save_dir'], cfg['exp_name'])\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "log_path = f\"{save_path}/log.txt\"\n",
    "if os.path.exists(log_path):\n",
    "    os.system(f\"rm {log_path}\")\n",
    "# 定义简单的日志方法\n",
    "log_fw = open(log_path, 'a+') # 打开日志文件保存日志\n",
    "def log(text):     # 定义一个日志记录函数来跟踪训练过程\n",
    "    print(text)\n",
    "    log_fw.write(str(text)+'\\n')\n",
    "    log_fw.flush()\n",
    "\n",
    "log(cfg)  # 写入配置"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "在本次作业中我们使用 Food11 数据集, 和homework3数据集相似，不过数据上稍微做了一些调整. 数据集可以直接在kaggle中载入，或者通过链接下载。\n",
    "\n",
    "```shell\n",
    "# 从github下载数据 (大约 1.12G)\n",
    "!wget https://github.com/virginiakm1988/ML2022-Spring/raw/main/HW13/food11-hw13.tar.gz\n",
    "# 备份链接:\n",
    "!wget https://github.com/andybi7676/ml2022spring-hw13/raw/main/food11-hw13.tar.gz -O food11-hw13.tar.gz\n",
    "# !gdown '1ijKoNmpike_yjUw8SWRVVWVoMOXXqycj' --output food11-hw13.tar.gz\n",
    "\n",
    "# 解压\n",
    "!tar -xzf ./food11-hw13.tar.gz \n",
    "# !tar -xzvf ./food11-hw13.tar.gz # 可以查看解压进度\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown '1ijKoNmpike_yjUw8SWRVVWVoMOXXqycj' --output food11-hw13.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../input/ml2022spring-hw13/food11-hw13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\deeplearning_dataset\\food11-hw13: 1 files.\n",
      "E:\\deeplearning_dataset\\food11-hw13\\evaluation: 3347 files.\n",
      "E:\\deeplearning_dataset\\food11-hw13\\training: 9866 files.\n",
      "E:\\deeplearning_dataset\\food11-hw13\\validation: 3430 files.\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('E:\\deeplearning_dataset\\\\food11-hw13'):\n",
    "    if len(filenames) > 0:\n",
    "        print(f\"{dirname}: {len(filenames)} files.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步, 特殊的train/test数据集变换进行数据扩增  \n",
    "Torchvision 提供了很多实用的图像预处理`image preprocessing`方法，数据扩增`data augmentation`方法\n",
    "\n",
    "可以参考 [PyTorch官方文档-transforms](https://pytorch.org/vision/stable/transforms.html) 了解不同的transforms方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# 定义 training/testing transforms\n",
    "test_tfm = transforms.Compose([\n",
    "    # 如果你正在使用提供的教师模型(teacher model)，则不建议修改此部分。\n",
    "    # 下列的transform 方法是标准的，并且足以进行测试。\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    # 在这里增加一些有用的transform 或 数据扩增方法, 基于你在HW3中学习的经验\n",
    "    transforms.Resize(256),  # 你可以修改这里\n",
    "    transforms.CenterCrop(224), # 你可以修改这里, 但是要注意，给定教师模型(teacher model)的输入大小是224。\n",
    "    # 因此，除了224之外的输入大小可能会降低模型性能。需要注意。\n",
    "    transforms.RandomHorizontalFlip(), # 你可以修改这里.\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, path, tfm=test_tfm, files=None):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.files = sorted([os.path.join(path, i) for i in os.listdir(path) if i.endswith('.jpg')])\n",
    "        if files is not None:\n",
    "            self.files = files\n",
    "        print(f'One {path} sample', self.files[0])\n",
    "        self.tfm = tfm\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        im = Image.open(fname)\n",
    "        im = self.tfm(im)\n",
    "        try:\n",
    "            # label = int(fname.split(\"/\")[-1].split('_')[0])    # windows和liunx文件路径区别\n",
    "            label = int(fname.split(\"\\\\\")[-1].split(\"_\")[0])  \n",
    "        except:\n",
    "            label = -1\n",
    "        return im, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One E:\\deeplearning_dataset\\food11-hw13\\training sample E:\\deeplearning_dataset\\food11-hw13\\training\\0_0.jpg\n",
      "One E:\\deeplearning_dataset\\food11-hw13\\validation sample E:\\deeplearning_dataset\\food11-hw13\\validation\\0_0.jpg\n"
     ]
    }
   ],
   "source": [
    "train_set = FoodDataset(os.path.join(cfg['dataset_root'], \"training\"), tfm=train_tfm)\n",
    "train_loader = DataLoader(train_set,batch_size=cfg['batch_size'], shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "val_set = FoodDataset(os.path.join(cfg['dataset_root'], \"validation\"), tfm=test_tfm)\n",
    "val_loader = DataLoader(val_set, batch_size=cfg['batch_size'], shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im, label= train_set.__getitem__(0)\n",
    "label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &#x2728; Architecture_Design\n",
    "\n",
    "在这个作业中我们需要设计一个更小的网络，并使它表现的十分良好。显然，一个好的网络结构的设计是十分关键的。  \n",
    "这里我们介绍深度`depthwise`和逐点`pointwise`卷积. 当涉及到网络压缩时， 这些变体的卷积架构设计是一些常见技术。\n",
    "\n",
    "- `depthwise`:\n",
    "    - 一个kenerl对一个channel\n",
    "    - in_channel == out_channel\n",
    "    - 缺点：无法捕捉channel之间的关系\n",
    "    \n",
    "- `pointwise`:\n",
    "    - `kernel_size=1`\n",
    "    - 仅仅考虑channel之间的关系\n",
    "    \n",
    "- `depthwise` + `pointwise`\n",
    "    - 参数减少 $\\frac{1}{O}+\\frac{1}{K\\times K}$ `O-输出channel, K-kernel大小`\n",
    "\n",
    "![dpdw](./HW13_pic/dwpw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例：Depthwise and Pointwise Convlution\n",
    "def dwpw_conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, groups=in_channels), # depthwise convolution\n",
    "        nn.Conv2d(in_channels, out_channels, 1) # pointwise convolution\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 其他有用的方法\n",
    "    - [group convolution](https://www.researchgate.net/figure/The-transformations-within-a-layer-in-DenseNets-left-and-CondenseNets-at-training-time_fig2_321325862)(实际上`depthwise convolution`是一种特殊的`group convolution`)\n",
    "    - [SqueezeNet](https://arxiv.org/abs/1602.07360)\n",
    "    - [MobileNet](https://arxiv.org/abs/1704.04861)\n",
    "    - [ShuffleNet](https://arxiv.org/abs/1707.01083)\n",
    "    - [Xception](https://arxiv.org/abs/1610.02357)\n",
    "    - [GhostNet](https://arxiv.org/abs/1911.11907)\n",
    " \n",
    "在介绍了深度卷积和点卷积之后，让我们定义**学生网络`student network`框架**。在这里，我们有一个由一些具有深度和逐点卷积的规则卷积层形成的简单网络。通过这种方式，你可以进一步增加网络的深度或宽度。\n",
    "\n",
    "<font color=darkred><b>TODO：修改成自己的网络框架</font></b>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里定义自己的 student network.\n",
    "# 我们将使用你的student network来评估您的结果（包括总参数量）\n",
    "\n",
    "class StudentNetOrg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentNetOrg, self).__init__()\n",
    "        # TODO: 修改成自己的网络框架\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            nn.Conv2d(64, 100, 3),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            # 在这里，我们对各种输入大小采用全局平均。\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(100, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentNet, self).__init__()\n",
    "        # TODO: 修改成自己的网络框架\n",
    "        self.cnn = nn.Sequential(\n",
    "            dwpw_conv(3, 32, 3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            dwpw_conv(32, 32, 3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            dwpw_conv(32, 64, 3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            dwpw_conv(64, 100, 3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "            \n",
    "            # 在这里，我们对各种输入大小采用全局平均。\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(100, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "def get_student_model():\n",
    "    return StudentNet()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确定`student network`框架后, 需要使用`torchsummary`获取网络的信息和验证参数总数. 需要注意`student network`网络参数总量，  \n",
    "网络参数的总量不能超过限制(`总参数（torchsummary中展示）<=100,000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "[ StudentNetOrg ]\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 222, 222]             896\n",
      "       BatchNorm2d-2         [-1, 32, 222, 222]              64\n",
      "              ReLU-3         [-1, 32, 222, 222]               0\n",
      "            Conv2d-4         [-1, 32, 220, 220]           9,248\n",
      "       BatchNorm2d-5         [-1, 32, 220, 220]              64\n",
      "              ReLU-6         [-1, 32, 220, 220]               0\n",
      "         MaxPool2d-7         [-1, 32, 110, 110]               0\n",
      "            Conv2d-8         [-1, 64, 108, 108]          18,496\n",
      "       BatchNorm2d-9         [-1, 64, 108, 108]             128\n",
      "             ReLU-10         [-1, 64, 108, 108]               0\n",
      "        MaxPool2d-11           [-1, 64, 54, 54]               0\n",
      "           Conv2d-12          [-1, 100, 52, 52]          57,700\n",
      "      BatchNorm2d-13          [-1, 100, 52, 52]             200\n",
      "             ReLU-14          [-1, 100, 52, 52]               0\n",
      "        MaxPool2d-15          [-1, 100, 26, 26]               0\n",
      "AdaptiveAvgPool2d-16            [-1, 100, 1, 1]               0\n",
      "           Linear-17                   [-1, 11]           1,111\n",
      "================================================================\n",
      "Total params: 87,907\n",
      "Trainable params: 87,907\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 99.72\n",
      "Params size (MB): 0.34\n",
      "Estimated Total Size (MB): 100.62\n",
      "----------------------------------------------------------------\n",
      "\n",
      "\n",
      "**********************************************************************\n",
      "[ StudentNet ]\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 3, 222, 222]              30\n",
      "            Conv2d-2         [-1, 32, 222, 222]             128\n",
      "       BatchNorm2d-3         [-1, 32, 222, 222]              64\n",
      "              ReLU-4         [-1, 32, 222, 222]               0\n",
      "            Conv2d-5         [-1, 32, 220, 220]             320\n",
      "            Conv2d-6         [-1, 32, 220, 220]           1,056\n",
      "       BatchNorm2d-7         [-1, 32, 220, 220]              64\n",
      "              ReLU-8         [-1, 32, 220, 220]               0\n",
      "         MaxPool2d-9         [-1, 32, 110, 110]               0\n",
      "           Conv2d-10         [-1, 32, 108, 108]             320\n",
      "           Conv2d-11         [-1, 64, 108, 108]           2,112\n",
      "      BatchNorm2d-12         [-1, 64, 108, 108]             128\n",
      "             ReLU-13         [-1, 64, 108, 108]               0\n",
      "        MaxPool2d-14           [-1, 64, 54, 54]               0\n",
      "           Conv2d-15           [-1, 64, 52, 52]             640\n",
      "           Conv2d-16          [-1, 100, 52, 52]           6,500\n",
      "      BatchNorm2d-17          [-1, 100, 52, 52]             200\n",
      "             ReLU-18          [-1, 100, 52, 52]               0\n",
      "        MaxPool2d-19          [-1, 100, 26, 26]               0\n",
      "AdaptiveAvgPool2d-20            [-1, 100, 1, 1]               0\n",
      "           Linear-21                   [-1, 11]           1,111\n",
      "================================================================\n",
      "Total params: 12,673\n",
      "Trainable params: 12,673\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 116.83\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 117.45\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "student_model= get_student_model()\n",
    "student_model_org = StudentNetOrg()\n",
    "print('**'*35)\n",
    "print(\"[ StudentNetOrg ]\")\n",
    "# 输出模型student_model_org的摘要信息，包括每一层的配置（如层的类型、输出形状、参数数量等），\n",
    "# 以及整个模型的参数总数和可训练参数的数量。\n",
    "# (3, 224, 224)指定了输入张量的形状，这里表示输入图像的通道数是3（RGB图像），高度和宽度都是224像素\n",
    "summary(student_model_org,  (3, 224, 224), device='cpu')\n",
    "print(\"\\n\")\n",
    "print('**'*35)\n",
    "print(\"[ StudentNet ]\")\n",
    "summary(student_model, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Lenovo/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 11]           5,643\n",
      "================================================================\n",
      "Total params: 11,182,155\n",
      "Trainable params: 11,182,155\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 42.66\n",
      "Estimated Total Size (MB): 106.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 载入提供的教师模型 (restnet18 num_classes=11, test-acc ~= 89.9%)\n",
    "teacher_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=11)\n",
    "# load_state dict\n",
    "teach_ckpt_path = os.path.join(cfg['dataset_root'], \"resnet18_teacher.ckpt\")\n",
    "teacher_model.load_state_dict(torch.load(teach_ckpt_path, map_location='cpu'))\n",
    "summary(teacher_model, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   &#x2728; Knowledge_Distillation\n",
    "\n",
    "既然我们有一个学习过的大模型，那就让它教另一个小模型吧。在实现中，让训练目标是大模型的预测，而不是实际标签\n",
    "\n",
    "\n",
    "**为什么这样能有效训练出小网络 ?**\n",
    "- 如果数据干净，那么大模型的预测可能会忽略带有错误标记的数据的噪声\n",
    "- 类之间可能存在一些关系，因此教师模型中的软标签可能会很有用。例如，数字8与6、9、0比1、7更相似\n",
    "\n",
    "**如何实施训练 ?**\n",
    "- 损失函数定义\n",
    "$$\\text{Loss} = \\alpha T^2 \\times KL(p||q) + (1-\\alpha)\\text{(Original Cross Entropy Loss)}$$\n",
    "\n",
    "    - $\\text{where p=softmax}(\\frac{\\text{student's logits}}{T})$\n",
    "    - $\\text{where q=softmax}(\\frac{\\text{teacher's logits}}{T})$\n",
    "    \n",
    "    \n",
    "- 使用链接: [`pytorch docs of KLDivLoss with examples` Link](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html)\n",
    "- 原始论文: [`Distilling the Knowledge in a Neural Network` Link](https://arxiv.org/abs/1503.02531)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred><b>TODO：参考上述的函数，结合`KL divergence Loss`和`CE Loss`完成损失函数的定义</font></b>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4162, -0.2073, -0.1003, -1.6474])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1127, 0.3776, 0.4202, 0.0895]),\n",
       " tensor([0.1279, 0.3659, 0.4016, 0.1046]),\n",
       " tensor([0.0374, 0.4195, 0.5196, 0.0235]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft = nn.Softmax(dim=-1)\n",
    "sft(a), sft(a/1.15), sft(a/0.5), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 KL divergence loss 实现知识蒸馏(know distillation)的损失函数 \n",
    "def loss_fn_kd(student_logits, labels, teacher_logits, alpha=0.5, temperature=1.15):\n",
    "    # temperature 越大越平滑\n",
    "    # TODO: \n",
    "    kl_loss = torch.nn.KLDivLoss(reduction='mean', log_target=True)\n",
    "    ce_loss = torch.nn.CrossEntropyLoss(reduction='mean',ignore_index=-1)\n",
    "    # 创建一个Softmax层，dim=-1表示在最后一个维度上应用Softmax\n",
    "    sft = nn.Softmax(dim=-1)\n",
    "    return alpha * temperature * temperature * kl_loss(sft(student_logits/temperature), sft(teacher_logits/temperature)) \\\n",
    "            + (1-alpha) * ce_loss(student_logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg['loss_fn_type']= KD\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"cfg['loss_fn_type']=\", cfg['loss_fn_type'])\n",
    "#  选择损失函数\n",
    "if cfg['loss_fn_type'] == 'CE':\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=-1) # simple base line\n",
    "\n",
    "if cfg['loss_fn_type'] == 'KD':\n",
    "    loss_fn = loss_fn_kd\n",
    "\n",
    "# 还可以自定义一些其他方法\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "log(f'device: {device}')\n",
    "device = torch.device(device)\n",
    "n_epochs = cfg['n_epochs']\n",
    "patience = cfg['patience']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-29T02:38:20.342506Z",
     "iopub.status.busy": "2023-06-29T02:38:20.342047Z",
     "iopub.status.idle": "2023-06-29T02:38:20.350732Z",
     "shell.execute_reply": "2023-06-29T02:38:20.349126Z",
     "shell.execute_reply.started": "2023-06-29T02:38:20.342474Z"
    }
   },
   "source": [
    "#   &#x2728; Training\n",
    "\n",
    "实现简单基线的训练循环，可以随意修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 001 / 020 ]:   0%|                                                             | 0/155 [00:00<?, ?it/s]G:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:2886: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "[ Train | Epoch 001 / 020 ]: 100%|████████████████████████████████████████| 155/155 [07:24<00:00,  2.87s/it, loss=62.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 001 / 020 ] loss = 1.13295 acc = 0.27002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 001 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [01:09<00:00,  1.30s/it, loss=57.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 001 / 020 ] loss = 1.04070 acc = 0.31370\n",
      "Best model found at epoch 1. saving model. acc=0.31370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 002 / 020 ]: 100%|████████████████████████████████████████| 155/155 [03:57<00:00,  1.54s/it, loss=57.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 002 / 020 ] loss = 1.02301 acc = 0.34604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 002 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [00:55<00:00,  1.02s/it, loss=49.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 002 / 020 ] loss = 0.98509 acc = 0.35539\n",
      "Best model found at epoch 2. saving model. acc=0.35539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 003 / 020 ]: 100%|████████████████████████████████████████| 155/155 [03:59<00:00,  1.54s/it, loss=57.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 003 / 020 ] loss = 0.98210 acc = 0.36935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 003 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [00:55<00:00,  1.03s/it, loss=52.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 003 / 020 ] loss = 0.95344 acc = 0.37085\n",
      "Best model found at epoch 3. saving model. acc=0.37085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 004 / 020 ]: 100%|██████████████████████████████████████████| 155/155 [03:59<00:00,  1.54s/it, loss=56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 004 / 020 ] loss = 0.95641 acc = 0.38840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 004 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [00:55<00:00,  1.03s/it, loss=48.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 004 / 020 ] loss = 0.93171 acc = 0.39446\n",
      "Best model found at epoch 4. saving model. acc=0.39446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 005 / 020 ]: 100%|████████████████████████████████████████| 155/155 [03:56<00:00,  1.53s/it, loss=54.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 005 / 020 ] loss = 0.93638 acc = 0.39864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 005 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [00:54<00:00,  1.01s/it, loss=45.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 005 / 020 ] loss = 0.92869 acc = 0.39096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 006 / 020 ]: 100%|████████████████████████████████████████| 155/155 [03:59<00:00,  1.55s/it, loss=55.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 006 / 020 ] loss = 0.91616 acc = 0.41456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 006 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [00:57<00:00,  1.06s/it, loss=48.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 006 / 020 ] loss = 0.90544 acc = 0.41603\n",
      "Best model found at epoch 6. saving model. acc=0.41603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 007 / 020 ]: 100%|████████████████████████████████████████| 155/155 [04:00<00:00,  1.55s/it, loss=53.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 007 / 020 ] loss = 0.90170 acc = 0.42702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 007 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [00:55<00:00,  1.02s/it, loss=42.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 007 / 020 ] loss = 0.90130 acc = 0.41603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 008 / 020 ]: 100%|████████████████████████████████████████| 155/155 [03:56<00:00,  1.53s/it, loss=50.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 008 / 020 ] loss = 0.88953 acc = 0.43503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 008 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [00:55<00:00,  1.03s/it, loss=46.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 008 / 020 ] loss = 0.87821 acc = 0.44548\n",
      "Best model found at epoch 8. saving model. acc=0.44548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 009 / 020 ]: 100%|████████████████████████████████████████| 155/155 [04:01<00:00,  1.56s/it, loss=52.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 009 / 020 ] loss = 0.87952 acc = 0.44273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 009 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [00:58<00:00,  1.08s/it, loss=51.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 009 / 020 ] loss = 0.88587 acc = 0.42711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 010 / 020 ]: 100%|████████████████████████████████████████| 155/155 [04:04<00:00,  1.58s/it, loss=51.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 010 / 020 ] loss = 0.87224 acc = 0.44932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 010 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [00:56<00:00,  1.05s/it, loss=44.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 010 / 020 ] loss = 0.85323 acc = 0.46093\n",
      "Best model found at epoch 10. saving model. acc=0.46093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 011 / 020 ]: 100%|████████████████████████████████████████| 155/155 [04:04<00:00,  1.58s/it, loss=51.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 011 / 020 ] loss = 0.85796 acc = 0.46047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 011 / 020 ]: 100%|██████████████████████████████████████████████| 54/54 [01:00<00:00,  1.11s/it, loss=43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 011 / 020 ] loss = 0.85979 acc = 0.46093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 012 / 020 ]: 100%|████████████████████████████████████████| 155/155 [04:02<00:00,  1.57s/it, loss=49.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 012 / 020 ] loss = 0.85038 acc = 0.46807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 012 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [00:57<00:00,  1.06s/it, loss=44.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 012 / 020 ] loss = 0.85236 acc = 0.45918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 013 / 020 ]: 100%|████████████████████████████████████████| 155/155 [04:05<00:00,  1.58s/it, loss=46.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 013 / 020 ] loss = 0.83914 acc = 0.47436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 013 / 020 ]: 100%|██████████████████████████████████████████████| 54/54 [00:56<00:00,  1.05s/it, loss=43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 013 / 020 ] loss = 0.83794 acc = 0.47201\n",
      "Best model found at epoch 13. saving model. acc=0.47201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 014 / 020 ]: 100%|████████████████████████████████████████| 155/155 [03:56<00:00,  1.53s/it, loss=49.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 014 / 020 ] loss = 0.83070 acc = 0.47861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 014 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [00:56<00:00,  1.04s/it, loss=43.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 014 / 020 ] loss = 0.84588 acc = 0.46968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 015 / 020 ]: 100%|████████████████████████████████████████| 155/155 [04:14<00:00,  1.64s/it, loss=49.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 015 / 020 ] loss = 0.82452 acc = 0.48307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 015 / 020 ]: 100%|██████████████████████████████████████████████| 54/54 [01:01<00:00,  1.13s/it, loss=46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 015 / 020 ] loss = 0.83806 acc = 0.46822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 016 / 020 ]: 100%|████████████████████████████████████████| 155/155 [04:04<00:00,  1.58s/it, loss=46.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 016 / 020 ] loss = 0.81530 acc = 0.48905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 016 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [00:59<00:00,  1.10s/it, loss=35.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 016 / 020 ] loss = 0.82606 acc = 0.47493\n",
      "Best model found at epoch 16. saving model. acc=0.47493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 017 / 020 ]: 100%|████████████████████████████████████████| 155/155 [04:22<00:00,  1.69s/it, loss=47.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 017 / 020 ] loss = 0.80895 acc = 0.49564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 017 / 020 ]: 100%|████████████████████████████████████████████| 54/54 [01:00<00:00,  1.12s/it, loss=45.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 017 / 020 ] loss = 0.83434 acc = 0.48076\n",
      "Best model found at epoch 17. saving model. acc=0.48076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 018 / 020 ]: 100%|████████████████████████████████████████| 155/155 [04:12<00:00,  1.63s/it, loss=46.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 018 / 020 ] loss = 0.80202 acc = 0.49959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 018 / 020 ]: 100%|██████████████████████████████████████████████| 54/54 [00:58<00:00,  1.09s/it, loss=41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 018 / 020 ] loss = 0.82064 acc = 0.47988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 019 / 020 ]: 100%|████████████████████████████████████████| 155/155 [04:13<00:00,  1.64s/it, loss=46.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 019 / 020 ] loss = 0.79574 acc = 0.50436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 019 / 020 ]: 100%|██████████████████████████████████████████████| 54/54 [00:59<00:00,  1.09s/it, loss=34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 019 / 020 ] loss = 0.81722 acc = 0.46618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Train | Epoch 020 / 020 ]: 100%|████████████████████████████████████████| 155/155 [04:14<00:00,  1.64s/it, loss=44.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 020 / 020 ] loss = 0.78810 acc = 0.51003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Val | Epoch 020 / 020 ]: 100%|██████████████████████████████████████████████| 54/54 [00:59<00:00,  1.10s/it, loss=34]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Val | 020 / 020 ] loss = 0.79626 acc = 0.49155\n",
      "Best model found at epoch 20. saving model. acc=0.49155\n",
      "Finish training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 模型初始化，并将参数移入训练设备\n",
    "student_model.to(device)\n",
    "opt = torch.optim.Adam(student_model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n",
    "\n",
    "# 初始化跟踪器, 这部分不是训练参数不需要修改\n",
    "stale = 0\n",
    "best_acc = 0.0\n",
    "\n",
    "teacher_model.to(device)\n",
    "teacher_model.eval() # MEDIUM BASELINE\n",
    "for epoch in range(n_epochs):\n",
    "    # ---------------- Training ----------------------\n",
    "    # 在训练之前，确保模型是开启训练模式的\n",
    "    student_model.train()\n",
    "    # 记录训练过程的信息\n",
    "    train_loss = []\n",
    "    train_accs = []\n",
    "    train_lens = []\n",
    "    tq_bar = tqdm(train_loader)\n",
    "    tq_bar.set_description(f\"[ Train | Epoch {epoch+1:03d} / {n_epochs:03d} ]\")\n",
    "    for imgs, labels in tq_bar:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # imgs = imgs.half() # 开启半精度。直接可以加快运行速度、减少GPU占用，并且只有不明显的accuracy损失。\n",
    "        # 前向传播\n",
    "        with torch.no_grad():  # MEDIUM BASELINE\n",
    "            teacher_logits = teacher_model(imgs)  # MEDIUM BASELINE\n",
    "        logits = student_model(imgs)\n",
    "        # 计算损失.\n",
    "        loss = loss_fn(logits, labels, teacher_logits)\n",
    "#         loss = loss_fn(logits, labels)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        acc = (logits.argmax(dim=-1) == labels).float().sum()\n",
    "        # 记录 loss 和 accuracy.\n",
    "        batch_len = len(imgs)\n",
    "        train_loss.append(loss.cpu().item() * batch_len)\n",
    "        train_accs.append(acc)\n",
    "        train_lens.append(batch_len)\n",
    "        tq_bar.set_postfix({\"loss\" : np.mean(train_loss[-10:])})\n",
    "    \n",
    "    train_loss = sum(train_loss)/sum(train_lens)\n",
    "    train_acc = sum(train_accs)/sum(train_lens)\n",
    "    # 打印信息\n",
    "    log(f'[ Train | {epoch+1:03d} / {n_epochs:03d} ] loss = {train_loss:.5f} acc = {train_acc:.5f}')\n",
    "    # ---------------- validation ----------------------\n",
    "    student_model.eval()\n",
    "    val_loss = []\n",
    "    val_accs = []\n",
    "    val_lens = []\n",
    "    tq_bar = tqdm(val_loader)\n",
    "    tq_bar.set_description(f\"[ Val | Epoch {epoch+1:03d} / {n_epochs:03d} ]\")\n",
    "    for imgs, labels in tq_bar:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 前向传播\n",
    "        with torch.no_grad():  # MEDIUM BASELINE\n",
    "            teacher_logits = teacher_model(imgs)  # MEDIUM BASELINE\n",
    "        with torch.no_grad():\n",
    "            logits = student_model(imgs)\n",
    "        loss = loss_fn(logits, labels, teacher_logits)\n",
    "#         loss = loss_fn(logits, labels)\n",
    "        # 1.使用argmax函数沿着最后一个维度（类别维度）找到最高分数的索引，这将给出预测的类别\n",
    "        # 2.将预测的类别与真实的标签进行比较，这将返回一个布尔张量，维度为[N]。如果预测正确，相应位置的元素为True，否则为False\n",
    "        # 3.将布尔张量转换为浮点张量，True变为1.0，False变为0.0\n",
    "        # 4.计算所有正确预测的总和，这将给出正确预测的数量。这里的sum()默认沿着所有维度进行求和，因此结果是单个数字\n",
    "        acc = (logits.argmax(dim=-1) == labels).float().sum()\n",
    "        # 记录 loss 和 accuracy.\n",
    "        batch_len = len(imgs)\n",
    "        val_loss.append(loss.cpu().item() * batch_len)\n",
    "        val_accs.append(acc)\n",
    "        val_lens.append(batch_len)\n",
    "        tq_bar.set_postfix({\"loss\" : np.mean(val_loss[-10:])})\n",
    "    \n",
    "    val_loss = sum(val_loss)/sum(val_lens)\n",
    "    val_acc = sum(val_accs)/sum(val_lens)\n",
    "    log(f'[ Val | {epoch+1:03d} / {n_epochs:03d} ] loss = {val_loss:.5f} acc = {val_acc:.5f}')\n",
    "    # 更新logs\n",
    "    if val_acc > best_acc:\n",
    "        log(f'Best model found at epoch {epoch+1}. saving model. acc={val_acc:.5f}')\n",
    "        best_acc = val_acc\n",
    "        torch.save(student_model.state_dict(), f\"{save_path}/student_best.ckpt\")\n",
    "        stale = 0\n",
    "    else:\n",
    "        stale += 1\n",
    "        if (stale > patience):\n",
    "            log(f'No improving {patience} consecutions. early stopping')\n",
    "            break\n",
    "    \n",
    "log(\"Finish training\")\n",
    "log_fw.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "载入训练好的最佳模型进行预测并生成`submission.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One E:\\deeplearning_dataset\\food11-hw13\\evaluation sample E:\\deeplearning_dataset\\food11-hw13\\evaluation\\0000.jpg\n"
     ]
    }
   ],
   "source": [
    "eval_set = FoodDataset(os.path.join(cfg['dataset_root'], \"evaluation\"), tfm=test_tfm)\n",
    "eval_loader = DataLoader(eval_set, batch_size=cfg['batch_size'], shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 53/53 [00:41<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# 载入模型\n",
    "student_model_best = get_student_model()\n",
    "ckpt_path = f\"{save_path}/student_best.ckpt\" \n",
    "student_model_best.load_state_dict(torch.load(ckpt_path, map_location='cpu'))\n",
    "student_model_best.to(device) \n",
    "\n",
    "# 开始评估\n",
    "student_model_best.eval()\n",
    "eval_preds = [] # storing predictions of the evaluation dataset\n",
    "\n",
    "for imgs, _ in tqdm(eval_loader):\n",
    "    # 在eval中不需要进行梯度下降\n",
    "    with torch.no_grad():\n",
    "        logits = student_model_best(imgs.to(device))\n",
    "        # 使用argmax函数沿着最后一个维度（类别维度）找到最高分数的索引，这将给出预测的类别\n",
    "        # squeeze函数用于移除所有大小为1的维度\n",
    "        # 这一步是可选的，只有在有额外的单维度时才需要\n",
    "        # 例如，如果logits的维度是[1, C]或者[N, 1]，那么这一步将分别把形状变为[C]和[N]\n",
    "        # 将预测类别的张量从GPU移动到CPU（如果它在GPU上）\n",
    "        # 将张量转换为NumPy数组\n",
    "        preds = list(logits.argmax(dim=-1).squeeze().cpu().numpy())\n",
    "\n",
    "    eval_preds += preds\n",
    "\n",
    "def pad4(i):\n",
    "    # 一个字符串格式化方法，用于将整数i转换为字符串，并确保该字符串至少有4个字符的长度。\n",
    "    # 使用zfill方法填充字符串，直到长度为4\n",
    "    # 如果字符串的长度小于4，那么在左侧填充0\n",
    "    # 如果字符串的长度已经是4或更长，那么不进行任何填充\n",
    "    return str(i).zfill(4)\n",
    "\n",
    "# 保存结果\n",
    "ids = [pad4(i) for i in range(0, len(eval_set))]\n",
    "categories = eval_preds\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Id'] = ids\n",
    "df['Category'] = categories\n",
    "df.to_csv(f\"submission.csv\", index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
